import collections
import re
import threading
from sqlalchemy.dialects.postgresql.base import PGDialect
from sqlalchemy.dialects.postgresql.psycopg2 import PGDialect_psycopg2
from sqlalchemy.util import warn
import sqlalchemy.sql as sql

import sqlalchemy.types as sqltypes

from .stmt_compiler import CockroachCompiler, CockroachIdentifierPreparer

# Map type names (as returned by information_schema) to sqlalchemy type
# objects.
#
# TODO(bdarnell): test more of these. The stock test suite only covers
# a few basic ones.
_type_map = dict(
    bool=sqltypes.BOOLEAN,  # introspection returns "BOOL" not boolean
    boolean=sqltypes.BOOLEAN,
    int=sqltypes.INT,
    integer=sqltypes.INT,
    smallint=sqltypes.INT,
    bigint=sqltypes.INT,
    float=sqltypes.FLOAT,
    real=sqltypes.FLOAT,
    double=sqltypes.FLOAT,
    decimal=sqltypes.DECIMAL,
    numeric=sqltypes.DECIMAL,
    date=sqltypes.DATE,
    timestamp=sqltypes.TIMESTAMP,
    timestamptz=sqltypes.TIMESTAMP,
    interval=sqltypes.Interval,
    string=sqltypes.VARCHAR,
    char=sqltypes.VARCHAR,
    varchar=sqltypes.VARCHAR,
    bytes=sqltypes.BLOB,
    blob=sqltypes.BLOB,
    json=sqltypes.JSON,
    jsonb=sqltypes.JSON,
)


class _SavepointState(threading.local):
    """Hack to override names used in savepoint statements.

    To get the Session to do the right thing with transaction retries,
    we use the begin_nested() method, which executes a savepoint. We
    need to transform the savepoint statements that are a part of this
    retry loop, while leaving other savepoints alone. Unfortunately
    the interface leaves us with no way to pass this information along
    except via a thread-local variable.
    """
    def __init__(self):
        self.cockroach_restart = False


savepoint_state = _SavepointState()


class CockroachDBDialect(PGDialect_psycopg2):
    name = 'cockroachdb'
    supports_comments = False
    supports_sequences = False
    statement_compiler = CockroachCompiler
    preparer = CockroachIdentifierPreparer

    def __init__(self, *args, **kwargs):
        super(CockroachDBDialect, self).__init__(*args,
                                                 use_native_hstore=False,
                                                 server_side_cursors=False,
                                                 **kwargs)

    def initialize(self, connection):
        # Bypass PGDialect's initialize implementation, which looks at
        # server_version_info and performs postgres-specific queries
        # to detect certain features on the server. Set the attributes
        # by hand and hope things don't change out from under us too
        # often.
        super(PGDialect, self).initialize(connection)
        self.implicit_returning = True
        self.supports_native_enum = False
        self.supports_smallserial = False
        self._backslash_escapes = False
        self._has_native_jsonb = True
        sversion = connection.scalar("select version()")
        self._is_v2plus = " v2." in sversion
        self._is_v21plus = self._is_v2plus and (" v2.0." not in sversion)
        self._has_native_json = self._is_v2plus
        self._has_native_jsonb = self._is_v2plus

    def _get_server_version_info(self, conn):
        # PGDialect expects a postgres server version number here,
        # although we've overridden most of the places where it's
        # used.
        return (9, 5, 0)

    def get_table_names(self, conn, schema=None, **kw):
        # Upstream implementation needs correlated subqueries.

        if not self._is_v2plus:
            # v1.1 or earlier.
            return [row.Table for row in conn.execute("SHOW TABLES")]

        # v2.0+ have a good information schema. Use it.
        return [row.table_name for row in conn.execute(
            "SELECT table_name FROM information_schema.tables WHERE table_schema=%s",
            (schema or self.default_schema_name,))]

    def has_table(self, conn, table, schema=None):
        # Upstream implementation needs pg_table_is_visible().
        return any(t == table for t in self.get_table_names(conn, schema=schema))

    # The upstream implementations of the reflection functions below depend on
    # correlated subqueries which are not yet supported.
    def get_columns(self, conn, table_name, schema=None, **kw):
        if not self._is_v2plus:
            # v1.1.
            # Bad: the table name is not properly escaped.
            # Oh well. Hoping 1.1 won't be around for long.
            rows = conn.execute('SHOW COLUMNS FROM "%s"."%s"' %
                                (schema or self.default_schema_name, table_name))
        else:
            # v2.0 or later. Information schema is usable.
            rows = conn.execute('''
        SELECT column_name, data_type, is_nullable::bool, column_default
        FROM information_schema.columns
        WHERE table_schema = %s AND table_name = %s''',
                                (schema or self.default_schema_name, table_name))

        res = []
        for row in rows:
            name, type_str, nullable, default = row[:4]
            # When there are type parameters, attach them to the
            # returned type object.
            m = re.match(r'^(\w+)(?:\(([0-9, ]*)\))?$', type_str)
            if m is None:
                warn("Could not parse type name '%s'" % type_str)
                typ = sqltypes.NULLTYPE()
            else:
                type_name, type_args = m.groups()
                try:
                    type_class = _type_map[type_name.lower()]
                except KeyError:
                    warn("Did not recognize type '%s' of column '%s'" %
                         (type_name, name))
                    type_class = sqltypes.NULLTYPE
                if type_args:
                    typ = type_class(*[int(s.strip()) for s in type_args.split(',')])
                else:
                    typ = type_class()
            res.append(dict(
                name=name,
                type=typ,
                nullable=nullable,
                default=default,
            ))
        return res

    def get_indexes(self, conn, table_name, schema=None, **kw):
        # Maps names to a bool indicating whether the index is unique.

        if not self._is_v2plus:
            # v1.1 or earlier.
            # Bad: the table name is not properly escaped.
            # Oh well. Hoping 1.1 won't be around for long.
            rows = conn.execute('''
SELECT "Name" as index_name,
       "Column" as column_name,
       "Unique" as unique,
       "Implicit" as implicit
FROM [SHOW INDEXES FROM "%s"."%s"]''' %
                                (schema or self.default_schema_name, table_name))
        else:
            # v2.0: usable information schema.
            rows = conn.execute('''
SELECT index_name, column_name, (not non_unique::bool) as unique, implicit::bool as implicit
FROM information_schema.statistics
WHERE table_schema = %s AND table_name = %s
        ''', (schema or self.default_schema_name, table_name))

        uniques = collections.OrderedDict()
        columns = collections.defaultdict(list)
        for row in rows:
            if row.implicit:
                continue
            columns[row.index_name].append(row.column_name)
            uniques[row.index_name] = row.unique
        res = []
        # Map over uniques because it preserves order.
        for name in uniques:
            res.append(dict(name=name, column_names=columns[name], unique=uniques[name]))
        return res

    def get_foreign_keys_v1(self, conn, table_name, schema=None, **kw):
        fkeys = []
        FK_REGEX = re.compile(
            r'(?P<referred_table>.+)?\.\[(?P<referred_columns>.+)?]')

        for row in conn.execute(
                'SHOW CONSTRAINTS FROM "%s"."%s"' %
                (schema or self.default_schema_name, table_name)):
            if row.Type.startswith("FOREIGN KEY"):
                m = re.search(FK_REGEX, row.Details)

                name = row.Name
                constrained_columns = row['Column(s)'].split(', ')
                referred_table = m.group('referred_table')
                referred_columns = m.group('referred_columns').split()
                referred_schema = schema
                fkey_d = {
                    'name': name,
                    'constrained_columns': constrained_columns,
                    'referred_table': referred_table,
                    'referred_columns': referred_columns,
                    'referred_schema': referred_schema
                }
                fkeys.append(fkey_d)
        return fkeys

    def get_foreign_keys(self, connection, table_name, schema=None,
                         postgresql_ignore_search_path=False, **kw):
        if not self._is_v2plus:
            # v1.1 or earlier.
            return self.get_foreign_keys_v1(connection, table_name, schema, **kw)

        # v2.0 or later.
        # This method is the same as the one in SQLAlchemy's pg dialect, with
        # a tweak to the FK regular expressions to tolerate whitespace between
        # the table name and the column list.
        # See also: https://github.com/cockroachdb/cockroach/issues/27123

        preparer = self.identifier_preparer
        table_oid = self.get_table_oid(connection, table_name, schema,
                                       info_cache=kw.get('info_cache'))

        FK_SQL = """
          SELECT r.conname,
                pg_catalog.pg_get_constraintdef(r.oid, true) as condef,
                n.nspname as conschema
          FROM  pg_catalog.pg_constraint r,
                pg_namespace n,
                pg_class c

          WHERE r.conrelid = :table AND
                r.contype = 'f' AND
                c.oid = confrelid AND
                n.oid = c.relnamespace
          ORDER BY 1
        """
        # http://www.postgresql.org/docs/9.0/static/sql-createtable.html
        FK_REGEX = re.compile(
            r'FOREIGN KEY \((.*?)\) REFERENCES (?:(.*?)\.)?(.*?)[\s]?\((.*?)\)'
            r'[\s]?(MATCH (FULL|PARTIAL|SIMPLE)+)?'
            r'[\s]?(ON UPDATE '
            r'(CASCADE|RESTRICT|NO ACTION|SET NULL|SET DEFAULT)+)?'
            r'[\s]?(ON DELETE '
            r'(CASCADE|RESTRICT|NO ACTION|SET NULL|SET DEFAULT)+)?'
            r'[\s]?(DEFERRABLE|NOT DEFERRABLE)?'
            r'[\s]?(INITIALLY (DEFERRED|IMMEDIATE)+)?'
        )

        t = sql.text(FK_SQL, typemap={
            'conname': sqltypes.Unicode,
            'condef': sqltypes.Unicode})
        c = connection.execute(t, table=table_oid)
        fkeys = []
        for conname, condef, conschema in c.fetchall():
            m = re.search(FK_REGEX, condef).groups()

            constrained_columns, referred_schema, \
                referred_table, referred_columns, \
                _, match, _, onupdate, _, ondelete, \
                deferrable, _, initially = m

            if deferrable is not None:
                deferrable = True if deferrable == 'DEFERRABLE' else False
            constrained_columns = [preparer._unquote_identifier(x)
                                   for x in re.split(
                                       r'\s*,\s*', constrained_columns)]

            if postgresql_ignore_search_path:
                # when ignoring search path, we use the actual schema
                # provided it isn't the "default" schema
                if conschema != self.default_schema_name:
                    referred_schema = conschema
                else:
                    referred_schema = schema
            elif referred_schema:
                # referred_schema is the schema that we regexp'ed from
                # pg_get_constraintdef().  If the schema is in the search
                # path, pg_get_constraintdef() will give us None.
                referred_schema = \
                    preparer._unquote_identifier(referred_schema)
            elif schema is not None and schema == conschema:
                # If the actual schema matches the schema of the table
                # we're reflecting, then we will use that.
                referred_schema = schema

            referred_table = preparer._unquote_identifier(referred_table)
            referred_columns = [preparer._unquote_identifier(x)
                                for x in
                                re.split(r'\s*,\s', referred_columns)]
            fkey_d = {
                'name': conname,
                'constrained_columns': constrained_columns,
                'referred_schema': referred_schema,
                'referred_table': referred_table,
                'referred_columns': referred_columns,
                'options': {
                    'onupdate': onupdate,
                    'ondelete': ondelete,
                    'deferrable': deferrable,
                    'initially': initially,
                    'match': match
                }
            }
            fkeys.append(fkey_d)
        return fkeys

    def get_pk_constraint(self, conn, table_name, schema=None, **kw):
        if self._is_v21plus:
            return super(CockroachDBDialect, self).get_pk_constraint(conn, table_name, schema, **kw)

        # v2.0 does not know about enough SQL to understand the query done by
        # the upstream dialect. So run a dumbed down version instead.
        idxs = self.get_indexes(conn, table_name, schema=schema, **kw)
        if len(idxs) == 0:
            # virtual table. No constraints.
            return {}
        # The PK is always first in the index list; it may not always
        # be named "primary".
        pk = idxs[0]
        res = dict(constrained_columns=pk["column_names"])
        # The SQLAlchemy tests expect that the name field is only
        # present if the PK was explicitly renamed by the user.
        # Checking for a name of "primary" is an imperfect proxy for
        # this but is good enough to pass the tests.
        if pk["name"] != "primary":
            res["name"] = pk["name"]
        return res

    def get_unique_constraints(self, conn, table_name, schema=None, **kw):
        if self._is_v21plus:
            return super(CockroachDBDialect, self).get_unique_constraints(
                conn, table_name, schema, **kw)

        # v2.0 does not know about enough SQL to understand the query done by
        # the upstream dialect. So run a dumbed down version instead.
        res = []
        # Skip the primary key which is always first in the list.
        idxs = self.get_indexes(conn, table_name, schema=schema, **kw)
        if len(idxs) == 0:
            # virtual table. No constraints.
            return res
        for index in idxs[1:]:
            if index["unique"]:
                del index["unique"]
                res.append(index)
        return res

    def get_check_constraints(self, conn, table_name, schema=None, **kw):
        if self._is_v21plus:
            return super(CockroachDBDialect, self).get_check_constraints(
                conn, table_name, schema, **kw)
        # TODO(bdarnell): The postgres dialect implementation depends on
        # pg_table_is_visible, which is supported in cockroachdb 1.1
        # but not in 1.0. Figure out a versioning strategy.
        return []

    def do_savepoint(self, connection, name):
        # Savepoint logic customized to work with run_transaction().
        if savepoint_state.cockroach_restart:
            connection.execute('SAVEPOINT cockroach_restart')
        else:
            super(CockroachDBDialect, self).do_savepoint(connection, name)

    def do_rollback_to_savepoint(self, connection, name):
        # Savepoint logic customized to work with run_transaction().
        if savepoint_state.cockroach_restart:
            connection.execute('ROLLBACK TO SAVEPOINT cockroach_restart')
        else:
            super(CockroachDBDialect, self).do_rollback_to_savepoint(connection, name)

    def do_release_savepoint(self, connection, name):
        # Savepoint logic customized to work with run_transaction().
        if savepoint_state.cockroach_restart:
            connection.execute('RELEASE SAVEPOINT cockroach_restart')
        else:
            super(CockroachDBDialect, self).do_release_savepoint(connection, name)


# If alembic is installed, register an alias in its dialect mapping.
try:
    import alembic.ddl.postgresql
except ImportError:
    pass
else:
    class CockroachDBImpl(alembic.ddl.postgresql.PostgresqlImpl):
        __dialect__ = 'cockroachdb'
        transactional_ddl = False


# If sqlalchemy-migrate is installed, register there too.
try:
    from migrate.changeset.databases.visitor import DIALECTS as migrate_dialects
except ImportError:
    pass
else:
    migrate_dialects['cockroachdb'] = migrate_dialects['postgresql']
